{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Setup\n",
    "# This section sets up the environment, installs necessary packages, and clones your GitHub repository.\n",
    "\n",
    "# Clone the GitHub repository\n",
    "!git clone https://github.com/JackRobert94464/RL-Honeypot.git\n",
    "%cd RL-Honeypot\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install -r https://raw.githubusercontent.com/JackRobert94464/RL-Honeypot/main/requirements_ubuntu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Inputs\n",
    "# This section takes inputs from the user for the FNR and FPR values, model choice, and training parameters.\n",
    "\n",
    "from google.colab import widgets\n",
    "\n",
    "# Create a text cell with the information for the user\n",
    "info_cell = widgets.Text(\"Please enter the following parameters:\")\n",
    "\n",
    "# Create input fields for FNR and FPR values\n",
    "fnr = float(input(\"Enter FNR value: \"))\n",
    "fpr = float(input(\"Enter FPR value: \"))\n",
    "\n",
    "# Ask the user which model they want to use for training\n",
    "print(\"Select the model for training:\")\n",
    "print(\"1: Standard Model\")\n",
    "print(\"2: Three Input Conv1D Model\")\n",
    "print(\"3: FNR/FPR Rate Model (Deprecated)\")\n",
    "model_choice = input(\"Enter the number of the model you want to train: \")\n",
    "model_name = None\n",
    "\n",
    "# Based on the user's choice, import the appropriate environment and agent\n",
    "if model_choice == '1':\n",
    "    model_name = \"Base\"\n",
    "    from NetworkHoneypotEnv_base import NetworkHoneypotEnv\n",
    "    from ddqn_agent_headless_v2 import DoubleDeepQLearning\n",
    "    print(\"Imported the environment and agent successfully.\")\n",
    "\n",
    "elif model_choice == '2':\n",
    "    model_name = \"3xConv1D\"\n",
    "    from MatrixTest3.test_3_NetworkHoneypotEnv import NetworkHoneypotEnv\n",
    "\n",
    "    print(\"Which input type would you like to use?\")\n",
    "    print(\"1: Single Input - quickly process observation state using one dense layer\")\n",
    "    print(\"2: Multi Input - a LTSM for observation state will be used\")\n",
    "    input_choice = input(\"Enter the number of the input type you want to use: \")\n",
    "\n",
    "    if input_choice == '1':\n",
    "        from MatrixTest3.ddqn_agent_3x_simple_state_fnrfpr import DoubleDeepQLearning\n",
    "    elif input_choice == '2':\n",
    "        from MatrixTest3.ddqn_agent_3x_multi_input_fnrfpr import DoubleDeepQLearning\n",
    "\n",
    "    print(\"Imported the environment and agent successfully.\")\n",
    "\n",
    "elif model_choice == '3':\n",
    "    model_name = \"FNRFPR\"\n",
    "    print(\"FNR/FPR Rate Model is deprecated. Please select another model.\")\n",
    "    print(\"15-05-2024 Both base and 3-input have been updated with fnr/fpr rates. Please select one of those.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Invalid selection. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Proceed with the training using the selected model\n",
    "print(\"Which training method would you like to use?\")\n",
    "print(\"1: Single Deception Node Training - Fixed number of Decoy Nodes\")\n",
    "print(\"2: Multiple Deception Node Training - Decremental number of Decoy Nodes, Longer Training\")\n",
    "training_choice = input(\"Enter the number of the training method you want to use: \")\n",
    "\n",
    "# Based on the user's choice, call the appropriate training function\n",
    "if training_choice == '1':\n",
    "    print(\"Enter the number of episodes you want to train for:\")\n",
    "    numberEpisodes = int(input(\"Enter the number of episodes: \"))\n",
    "    print(\"And how many decoy nodes will be available?\")\n",
    "    deception_nodes = int(input(\"Enter the number of deception nodes: \"))\n",
    "elif training_choice == '2':\n",
    "    print(\"Enter the number of episodes you want to train for each number of decoy nodes:\")\n",
    "    print(\"BEWARE: Try to keep this low as the training will take a long time if there's many counts of decoy nodes.\")\n",
    "    numberEpisodes = int(input(\"Enter the number of episodes: \"))\n",
    "else:\n",
    "    print(\"Invalid selection. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Driver Code\n",
    "# This section contains the main training logic.\n",
    "\n",
    "import os\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from NetworkHoneypotEnv_base import NetworkHoneypotEnv\n",
    "import evaluation_v2\n",
    "import misc\n",
    "\n",
    "# Defining parameters\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "epsilon = 0.1\n",
    "\n",
    "# Load the TPG data\n",
    "if os.name == 'nt':  # If the operating system is Windows\n",
    "    ntpg = misc.create_dictionary_ntpg(\".\\\\Development\\\\TPG-Data\\\\ntpg_big.csv\")\n",
    "    htpg = misc.create_dictionary_htpg(\".\\\\Development\\\\TPG-Data\\\\htpg_big.csv\")\n",
    "else:  # For other operating systems like Linux\n",
    "    ntpg = misc.create_dictionary_ntpg(\"./Development/TPG-Data/ntpg.csv\")\n",
    "    htpg = misc.create_dictionary_htpg(\"./Development/TPG-Data/htpg.csv\")\n",
    "\n",
    "normal_nodes = misc.count_nodes(ntpg)\n",
    "print(\"Normal nodes:\", normal_nodes)\n",
    "\n",
    "def TestTrain():\n",
    "    '''\n",
    "    Short training for testing out the dsp graphing function\n",
    "    \n",
    "    This func is use for the main which is now currently developing the modular trainingEpisode function (13/05/2024)\n",
    "    \n",
    "    For debugging purpose, uncomment this\n",
    "    '''\n",
    "    \n",
    "    # Initialize empty dictionaries to store the training time and DSP values\n",
    "    training_time_dict = {}\n",
    "    dsp_dict = {}\n",
    "\n",
    "    deception_nodes = 2 # Change this to the number of deception nodes you want to test\n",
    "\n",
    "    first_parameter = misc.calculate_first_parameter(deception_nodes, normal_nodes)\n",
    "\n",
    "    # Create the environment\n",
    "    env = NetworkHoneypotEnv(first_parameter, deception_nodes, normal_nodes, ntpg, htpg)\n",
    "\n",
    "    # Create the environment. Since it was built using PyEnvironment, we need to wrap it in a TFEnvironment to use with TF-Agents\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "    timestep = tf_env.reset()\n",
    "    rewards = []\n",
    "    numberEpisodes = 20000\n",
    "\n",
    "    # calculate the number of possible combinations\n",
    "    total_permutations = misc.calculate_permutation(normal_nodes, deception_nodes)\n",
    "\n",
    "    # create an object\n",
    "    LearningQDeep = DoubleDeepQLearning(env, gamma, epsilon, numberEpisodes, normal_nodes, total_permutations, fnr, fpr)\n",
    "    \n",
    "    # Training the Agent with a fixed number of episodes\n",
    "    for ep in range(numberEpisodes):\n",
    "        LearningQDeep.updateTrainingEpisode(ep)\n",
    "        LearningQDeep.trainingSingleEpisodes()\n",
    "        \n",
    "        # Every 2000, 5000, 10000, step, we perform evaluation        \n",
    "        currentStep = LearningQDeep.getStepCount()\n",
    "        \n",
    "        if currentStep in [2000, 5000, 10000, 20000, 30000]:\n",
    "            \n",
    "            # Initialize an evalutaion instance\n",
    "            evaluator = evaluation_v2.Evaluation()\n",
    "            \n",
    "            # Save models to folder\n",
    "            LearningQDeep.saveModel()\n",
    "            model_path = LearningQDeep.retrieveModelPath()\n",
    "            \n",
    "            # Collect the training time dict from training code\n",
    "            training_time_dict.update(LearningQDeep.retrieveTraintimeDict())\n",
    "            \n",
    "            # Evaluate the model\n",
    "            evaluator.evaluate(LearningQDeep, model_path)\n",
    "            \n",
    "            # Collect the DSP dict from evaluation code\n",
    "            dsp_dict.update(evaluator.retrieveDSPdict())\n",
    "            \n",
    "            # Save the training time dict and DSP dict to a text file\n",
    "            with open(f\"result_fnr{fnr}_fpr{fpr}.txt\", \"w\") as file:\n",
    "                file.write(f\"Training Time Dict: {training_time_dict}\\n\")\n",
    "                file.write(f\"DSP Dict: {dsp_dict}\\n\")\n",
    "    \n",
    "    import ddqn_dsp_visualizer\n",
    "    import ddqn_trainingtime_visualizer\n",
    "\n",
    "    print(\"Total steps: \", LearningQDeep.getGlobalStepCount())\n",
    "    print(\"Total DSP: \", LearningQDeep.getGlobalDSPCount())\n",
    "    print(\"Total Time: \", LearningQDeep.getGlobalTimeTaken())\n",
    "\n",
    "    # Visualize the Defense Success Probability (DSP) of our method\n",
    "    with open(f\"result_fnr{fnr}_fpr{fpr}.txt\", \"w\") as file:\n",
    "        file.write(f\"Global Step Count: {LearningQDeep.getGlobalStepCount()}\\n\")\n",
    "        file.write(f\"Global DSP Count: {LearningQDeep.getGlobalDSPCount()}\\n\")\n",
    "    ddqn_dsp_visualizer.ddqn_dsp_visual(LearningQDeep.getGlobalStepCount(), LearningQDeep.getGlobalDSPCount())\n",
    "\n",
    "    # Visualize the training time taken of our method\n",
    "    ddqn_trainingtime_visualizer.ddqn_dsp_visual(LearningQDeep.getGlobalStepCount(), LearningQDeep.getGlobalTimeTaken())\n",
    "\n",
    "    # Get the obtained rewards in every episode\n",
    "    LearningQDeep.sumRewardsEpisode\n",
    "    print(rewards)\n",
    "\n",
    "    # Summarize the model\n",
    "    LearningQDeep.mainNetwork.summary()\n",
    "    # Save the model\n",
    "    if os.name == 'nt':  # If the operating system is Windows\n",
    "        LearningQDeep.mainNetwork.save(f\".\\\\TrainedModel\\\\weighted_random_attacker\\\\RL_Honeypot_weighted_attacker_1to5_decoy_win_ver{numberEpisodes}_fnrfpr_{fnr}{fpr}.keras\")\n",
    "    else:  # For other operating systems like Linux\n",
    "        LearningQDeep.mainNetwork.save(f\"./TrainedModel/weighted_random_attacker/RL_Honeypot_weighted_attacker_1to5_decoy_linux_ver{numberEpisodes}_fnrfpr_{fnr}{fpr}.keras\")\n",
    "\n",
    "def SingleDecoyTraining(deception_nodes, numberEpisodes, model_name):\n",
    "    '''\n",
    "    Short training for testing out the dsp graphing function\n",
    "    For debugging purpose, uncomment this\n",
    "    '''\n",
    "\n",
    "    first_parameter = misc.calculate_first_parameter(deception_nodes, normal_nodes)\n",
    "\n",
    "    # Create the environment\n",
    "    env = NetworkHoneypotEnv(first_parameter, deception_nodes, normal_nodes, ntpg, htpg)\n",
    "\n",
    "    numberEpisodes = numberEpisodes\n",
    "\n",
    "    # Calculate the number of possible combinations\n",
    "    total_permutations = misc.calculate_permutation(normal_nodes, deception_nodes)\n",
    "\n",
    "    # Initialize empty dictionaries to store the training time and DSP values\n",
    "    training_time_dict = {}\n",
    "    dsp_dict = {}\n",
    "    \n",
    "    # Create an object\n",
    "    LearningQDeep = DoubleDeepQLearning(env, gamma, epsilon, numberEpisodes, normal_nodes, total_permutations, fnr, fpr)\n",
    "    \n",
    "    # Run the learning process\n",
    "    for ep in range(numberEpisodes):\n",
    "        LearningQDeep.updateTrainingEpisode(ep)\n",
    "        LearningQDeep.trainingSingleEpisodes()\n",
    "        \n",
    "        # Every 2000, 5000, 10000, step, we perform evaluation        \n",
    "        currentStep = LearningQDeep.getStepCount()\n",
    "        \n",
    "        if currentStep in [2000, 5000, 10000, 20000, 30000]:\n",
    "            \n",
    "            # Initialize an evalutaion instance\n",
    "            evaluator = evaluation_v2.Evaluation()\n",
    "            \n",
    "            # Save models to folder\n",
    "            LearningQDeep.saveModel()\n",
    "            model_path = LearningQDeep.retrieveModelPath()\n",
    "            \n",
    "            # Collect the training time dict from training code\n",
    "            training_time_dict.update(LearningQDeep.retrieveTraintimeDict())\n",
    "            \n",
    "            # Evaluate the model\n",
    "            evaluator.evaluate(model_path)\n",
    "            \n",
    "            # Collect the DSP dict from evaluation code\n",
    "            dsp_dict.update(evaluator.retrieveDSPdict())\n",
    "            \n",
    "            # Save the training time dict and DSP dict to a text file\n",
    "            with open(f\"result_fnr{fnr}_fpr{fpr}_model_{model_name}.txt\", \"w\") as file:\n",
    "                file.write(f\"Training Time Dict: {training_time_dict}\\n\")\n",
    "                file.write(f\"DSP Dict: {dsp_dict}\\n\")\n",
    "                \n",
    "            print(\"Training Time and DSP saved to file.\")\n",
    "            print(f\"File name: result_fnr{fnr}_fpr{fpr}_model_{model_name}.txt\")\n",
    "\n",
    "    # Summarize the model\n",
    "    LearningQDeep.mainNetwork.summary()\n",
    "\n",
    "def MultiDecoyTraining(numberEpisodes, model_name):\n",
    "    '''\n",
    "    For loop for long training\n",
    "    The training will start from giving the agent only 1 deception node and increase the number of deception nodes by 1 in each iteration.\n",
    "    The training will stop when the number of deception nodes is equal to half of the number of normal nodes.\n",
    "    '''\n",
    "    for i in range(normal_nodes // 2 + 1, 0, -1):\n",
    "        deception_nodes = i\n",
    "        \n",
    "        first_parameter = misc.calculate_first_parameter(deception_nodes, normal_nodes)\n",
    "\n",
    "        # Create the environment\n",
    "        env = NetworkHoneypotEnv(first_parameter, deception_nodes, normal_nodes, ntpg, htpg)\n",
    "\n",
    "        numberEpisodes = numberEpisodes\n",
    "\n",
    "        # Calculate the number of possible combinations\n",
    "        total_permutations = misc.calculate_permutation(normal_nodes, deception_nodes)\n",
    "\n",
    "        # Initialize empty dictionaries to store the training time and DSP values\n",
    "        training_time_dict = {}\n",
    "        dsp_dict = {}\n",
    "\n",
    "        # Create an object\n",
    "        LearningQDeep = DoubleDeepQLearning(env, gamma, epsilon, numberEpisodes, normal_nodes, total_permutations, fnr, fpr)\n",
    "        \n",
    "        # Run the learning process\n",
    "        for ep in range(numberEpisodes):\n",
    "            LearningQDeep.updateTrainingEpisode(ep)\n",
    "            LearningQDeep.trainingSingleEpisodes()\n",
    "            \n",
    "            # Every 2000, 5000, 10000, step, we perform evaluation        \n",
    "            currentStep = LearningQDeep.getStepCount()\n",
    "            \n",
    "            if currentStep in [2000, 5000, 10000, 20000, 30000]:\n",
    "                \n",
    "                # Initialize an evalutaion instance\n",
    "                evaluator = evaluation_v2.Evaluation()\n",
    "                \n",
    "                # Save models to folder\n",
    "                LearningQDeep.saveModel()\n",
    "                model_path = LearningQDeep.retrieveModelPath()\n",
    "                \n",
    "                # Collect the training time dict from training code\n",
    "                training_time_dict.update(LearningQDeep.retrieveTraintimeDict())\n",
    "                \n",
    "                # Evaluate the model\n",
    "                evaluator.evaluate(model_path)\n",
    "                \n",
    "                # Collect the DSP dict from evaluation code\n",
    "                dsp_dict.update(evaluator.retrieveDSPdict())\n",
    "                \n",
    "                # Save the training time dict and DSP dict to a text file\n",
    "                with open(f\"result_fnr{fnr}_fpr{fpr}_model_{model_name}.txt\", \"w\") as file:\n",
    "                    file.write(f\"Training Time Dict: {training_time_dict}\\n\")\n",
    "                    file.write(f\"DSP Dict: {dsp_dict}\\n\")\n",
    "                \n",
    "                print(\"Training Time and DSP saved to file.\")\n",
    "                print(f\"File name: result_fnr{fnr}_fpr{fpr}_model_{model_name}.txt\")\n",
    "            \n",
    "        # Summarize the model\n",
    "        LearningQDeep.mainNetwork.summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if training_choice == '1':\n",
    "        SingleDecoyTraining(deception_nodes, numberEpisodes, model_name)\n",
    "    elif training_choice == '2':\n",
    "        MultiDecoyTraining(numberEpisodes, model_name)\n",
    "    else:\n",
    "        print(\"Invalid selection. Exiting.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
